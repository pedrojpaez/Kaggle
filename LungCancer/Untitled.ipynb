{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luna node detection Kaggle tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import csv\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "try:\n",
    "    from tqdm import tqdm # long waits are not fun\n",
    "except:\n",
    "    print('TQDM does make much nicer wait bars...')\n",
    "    tqdm = lambda x: x\n",
    "\n",
    "#Some helper functions\n",
    "\n",
    "def make_mask(center,diam,z,width,height,spacing,origin):\n",
    "    '''\n",
    "Center : centers of circles px -- list of coordinates x,y,z\n",
    "diam : diameters of circles px -- diameter\n",
    "widthXheight : pixel dim of image\n",
    "spacing = mm/px conversion rate np array x,y,z\n",
    "origin = x,y,z mm np.array\n",
    "z = z position of slice in world coordinates mm\n",
    "    '''\n",
    "    mask = np.zeros([height,width]) # 0's everywhere except nodule swapping x,y to match img\n",
    "    #convert to nodule space from world coordinates\n",
    "\n",
    "    # Defining the voxel range in which the nodule falls\n",
    "    v_center = (center-origin)/spacing\n",
    "    v_diam = int(diam/spacing[0]+5)\n",
    "    v_xmin = np.max([0,int(v_center[0]-v_diam)-5])\n",
    "    v_xmax = np.min([width-1,int(v_center[0]+v_diam)+5])\n",
    "    v_ymin = np.max([0,int(v_center[1]-v_diam)-5]) \n",
    "    v_ymax = np.min([height-1,int(v_center[1]+v_diam)+5])\n",
    "\n",
    "    v_xrange = range(v_xmin,v_xmax+1)\n",
    "    v_yrange = range(v_ymin,v_ymax+1)\n",
    "\n",
    "    # Convert back to world coordinates for distance calculation\n",
    "    x_data = [x*spacing[0]+origin[0] for x in range(width)]\n",
    "    y_data = [x*spacing[1]+origin[1] for x in range(height)]\n",
    "\n",
    "    # Fill in 1 within sphere around nodule\n",
    "    for v_x in v_xrange:\n",
    "        for v_y in v_yrange:\n",
    "            p_x = spacing[0]*v_x + origin[0]\n",
    "            p_y = spacing[1]*v_y + origin[1]\n",
    "            if np.linalg.norm(center-np.array([p_x,p_y,z]))<=diam:\n",
    "                mask[int((p_y-origin[1])/spacing[1]),int((p_x-origin[0])/spacing[0])] = 1.0\n",
    "    return(mask)\n",
    "\n",
    "def matrix2int16(matrix):\n",
    "    ''' \n",
    "matrix must be a numpy array NXN\n",
    "Returns uint16 version\n",
    "    '''\n",
    "    m_min= np.min(matrix)\n",
    "    m_max= np.max(matrix)\n",
    "    matrix = matrix-m_min\n",
    "    return(np.array(np.rint( (matrix-m_min)/float(m_max-m_min) * 65535.0),dtype=np.uint16))\n",
    "\n",
    "############\n",
    "#\n",
    "# Getting list of image files\n",
    "luna_path = \"/home/jonathan/LUNA2016/\"\n",
    "luna_subset_path = luna_path+\"subset1/\"\n",
    "output_path = \"/home/jonathan/tutorial/\"\n",
    "file_list=glob(luna_subset_path+\"*.mhd\")\n",
    "\n",
    "\n",
    "#####################\n",
    "#\n",
    "# Helper function to get rows in data frame associated \n",
    "# with each file\n",
    "def get_filename(file_list, case):\n",
    "    for f in file_list:\n",
    "        if case in f:\n",
    "            return(f)\n",
    "#\n",
    "# The locations of the nodes\n",
    "df_node = pd.read_csv(luna_path+\"annotations.csv\")\n",
    "df_node[\"file\"] = df_node[\"seriesuid\"].map(lambda file_name: get_filename(file_list, file_name))\n",
    "df_node = df_node.dropna()\n",
    "\n",
    "#####\n",
    "#\n",
    "# Looping over the image files\n",
    "#\n",
    "for fcount, img_file in enumerate(tqdm(file_list)):\n",
    "    mini_df = df_node[df_node[\"file\"]==img_file] #get all nodules associate with file\n",
    "    if mini_df.shape[0]>0: # some files may not have a nodule--skipping those \n",
    "        # load the data once\n",
    "        itk_img = sitk.ReadImage(img_file) \n",
    "        img_array = sitk.GetArrayFromImage(itk_img) # indexes are z,y,x (notice the ordering)\n",
    "        num_z, height, width = img_array.shape        #heightXwidth constitute the transverse plane\n",
    "        origin = np.array(itk_img.GetOrigin())      # x,y,z  Origin in world coordinates (mm)\n",
    "        spacing = np.array(itk_img.GetSpacing())    # spacing of voxels in world coor. (mm)\n",
    "        # go through all nodes (why just the biggest?)\n",
    "        for node_idx, cur_row in mini_df.iterrows():       \n",
    "            node_x = cur_row[\"coordX\"]\n",
    "            node_y = cur_row[\"coordY\"]\n",
    "            node_z = cur_row[\"coordZ\"]\n",
    "            diam = cur_row[\"diameter_mm\"]\n",
    "            # just keep 3 slices\n",
    "            imgs = np.ndarray([3,height,width],dtype=np.float32)\n",
    "            masks = np.ndarray([3,height,width],dtype=np.uint8)\n",
    "            center = np.array([node_x, node_y, node_z])   # nodule center\n",
    "            v_center = np.rint((center-origin)/spacing)  # nodule center in voxel space (still x,y,z ordering)\n",
    "            for i, i_z in enumerate(np.arange(int(v_center[2])-1,\n",
    "                             int(v_center[2])+2).clip(0, num_z-1)): # clip prevents going out of bounds in Z\n",
    "                mask = make_mask(center, diam, i_z*spacing[2]+origin[2],\n",
    "                                 width, height, spacing, origin)\n",
    "                masks[i] = mask\n",
    "                imgs[i] = img_array[i_z]\n",
    "            np.save(os.path.join(output_path,\"images_%04d_%04d.npy\" % (fcount, node_idx)),imgs)\n",
    "            np.save(os.path.join(output_path,\"masks_%04d_%04d.npy\" % (fcount, node_idx)),masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import morphology\n",
    "from skimage import measure\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.transform import resize\n",
    "from glob import glob\n",
    "\n",
    "working_path = \"/home/jonathan/tutorial/\"\n",
    "file_list=glob(working_path+\"images_*.npy\")\n",
    "\n",
    "for img_file in file_list:\n",
    "    # I ran into an error when using Kmean on np.float16, so I'm using np.float64 here\n",
    "    imgs_to_process = np.load(img_file).astype(np.float64) \n",
    "    print \"on image\", img_file\n",
    "    for i in range(len(imgs_to_process)):\n",
    "        img = imgs_to_process[i]\n",
    "        #Standardize the pixel values\n",
    "        mean = np.mean(img)\n",
    "        std = np.std(img)\n",
    "        img = img-mean\n",
    "        img = img/std\n",
    "        # Find the average pixel value near the lungs\n",
    "        # to renormalize washed out images\n",
    "        middle = img[100:400,100:400] \n",
    "        mean = np.mean(middle)  \n",
    "        max = np.max(img)\n",
    "        min = np.min(img)\n",
    "        # To improve threshold finding, I'm moving the \n",
    "        # underflow and overflow on the pixel spectrum\n",
    "        img[img==max]=mean\n",
    "        img[img==min]=mean\n",
    "        #\n",
    "        # Using Kmeans to separate foreground (radio-opaque tissue)\n",
    "        # and background (radio transparent tissue ie lungs)\n",
    "        # Doing this only on the center of the image to avoid \n",
    "        # the non-tissue parts of the image as much as possible\n",
    "        #\n",
    "        kmeans = KMeans(n_clusters=2).fit(np.reshape(middle,[np.prod(middle.shape),1]))\n",
    "        centers = sorted(kmeans.cluster_centers_.flatten())\n",
    "        threshold = np.mean(centers)\n",
    "        thresh_img = np.where(img<threshold,1.0,0.0)  # threshold the image\n",
    "        #\n",
    "        # I found an initial erosion helful for removing graininess from some of the regions\n",
    "        # and then large dialation is used to make the lung region \n",
    "        # engulf the vessels and incursions into the lung cavity by \n",
    "        # radio opaque tissue\n",
    "        #\n",
    "        eroded = morphology.erosion(thresh_img,np.ones([4,4]))\n",
    "        dilation = morphology.dilation(eroded,np.ones([10,10]))\n",
    "        #\n",
    "        #  Label each region and obtain the region properties\n",
    "        #  The background region is removed by removing regions \n",
    "        #  with a bbox that is to large in either dimnsion\n",
    "        #  Also, the lungs are generally far away from the top \n",
    "        #  and bottom of the image, so any regions that are too\n",
    "        #  close to the top and bottom are removed\n",
    "        #  This does not produce a perfect segmentation of the lungs\n",
    "        #  from the image, but it is surprisingly good considering its\n",
    "        #  simplicity. \n",
    "        #\n",
    "        labels = measure.label(dilation)\n",
    "        label_vals = np.unique(labels)\n",
    "        regions = measure.regionprops(labels)\n",
    "        good_labels = []\n",
    "        for prop in regions:\n",
    "            B = prop.bbox\n",
    "            if B[2]-B[0]<475 and B[3]-B[1]<475 and B[0]>40 and B[2]<472:\n",
    "                good_labels.append(prop.label)\n",
    "        mask = np.ndarray([512,512],dtype=np.int8)\n",
    "        mask[:] = 0\n",
    "        #\n",
    "        #  The mask here is the mask for the lungs--not the nodes\n",
    "        #  After just the lungs are left, we do another large dilation\n",
    "        #  in order to fill in and out the lung mask \n",
    "        #\n",
    "        for N in good_labels:\n",
    "            mask = mask + np.where(labels==N,1,0)\n",
    "        mask = morphology.dilation(mask,np.ones([10,10])) # one last dilation\n",
    "        imgs_to_process[i] = mask\n",
    "    np.save(img_file.replace(\"images\",\"lungmask\"),imgs_to_process)\n",
    "    \n",
    "\n",
    "#\n",
    "#    Here we're applying the masks and cropping and resizing the image\n",
    "#\n",
    "\n",
    "\n",
    "file_list=glob(working_path+\"lungmask_*.npy\")\n",
    "out_images = []      #final set of images\n",
    "out_nodemasks = []   #final set of nodemasks\n",
    "for fname in file_list:\n",
    "    print \"working on file \", fname\n",
    "    imgs_to_process = np.load(fname.replace(\"lungmask\",\"images\"))\n",
    "    masks = np.load(fname)\n",
    "    node_masks = np.load(fname.replace(\"lungmask\",\"masks\"))\n",
    "    for i in range(len(imgs_to_process)):\n",
    "        mask = masks[i]\n",
    "        node_mask = node_masks[i]\n",
    "        img = imgs_to_process[i]\n",
    "        new_size = [512,512]   # we're scaling back up to the original size of the image\n",
    "        img= mask*img          # apply lung mask\n",
    "        #\n",
    "        # renormalizing the masked image (in the mask region)\n",
    "        #\n",
    "        new_mean = np.mean(img[mask>0])  \n",
    "        new_std = np.std(img[mask>0])\n",
    "        #\n",
    "        #  Pulling the background color up to the lower end\n",
    "        #  of the pixel range for the lungs\n",
    "        #\n",
    "        old_min = np.min(img)       # background color\n",
    "        img[img==old_min] = new_mean-1.2*new_std   # resetting backgound color\n",
    "        img = img-new_mean\n",
    "        img = img/new_std\n",
    "        #make image bounding box  (min row, min col, max row, max col)\n",
    "        labels = measure.label(mask)\n",
    "        regions = measure.regionprops(labels)\n",
    "        #\n",
    "        # Finding the global min and max row over all regions\n",
    "        #\n",
    "        min_row = 512\n",
    "        max_row = 0\n",
    "        min_col = 512\n",
    "        max_col = 0\n",
    "        for prop in regions:\n",
    "            B = prop.bbox\n",
    "            if min_row > B[0]:\n",
    "                min_row = B[0]\n",
    "            if min_col > B[1]:\n",
    "                min_col = B[1]\n",
    "            if max_row < B[2]:\n",
    "                max_row = B[2]\n",
    "            if max_col < B[3]:\n",
    "                max_col = B[3]\n",
    "        width = max_col-min_col\n",
    "        height = max_row - min_row\n",
    "        if width > height:\n",
    "            max_row=min_row+width\n",
    "        else:\n",
    "            max_col = min_col+height\n",
    "        # \n",
    "        # cropping the image down to the bounding box for all regions\n",
    "        # (there's probably an skimage command that can do this in one line)\n",
    "        # \n",
    "        img = img[min_row:max_row,min_col:max_col]\n",
    "        mask =  mask[min_row:max_row,min_col:max_col]\n",
    "        if max_row-min_row <5 or max_col-min_col<5:  # skipping all images with no god regions\n",
    "            pass\n",
    "        else:\n",
    "            # moving range to -1 to 1 to accomodate the resize function\n",
    "            mean = np.mean(img)\n",
    "            img = img - mean\n",
    "            min = np.min(img)\n",
    "            max = np.max(img)\n",
    "            img = img/(max-min)\n",
    "            new_img = resize(img,[512,512])\n",
    "            new_node_mask = resize(node_mask[min_row:max_row,min_col:max_col],[512,512])\n",
    "            out_images.append(new_img)\n",
    "            out_nodemasks.append(new_node_mask)\n",
    "\n",
    "num_images = len(out_images)\n",
    "#\n",
    "#  Writing out images and masks as 1 channel arrays for input into network\n",
    "#\n",
    "final_images = np.ndarray([num_images,1,512,512],dtype=np.float32)\n",
    "final_masks = np.ndarray([num_images,1,512,512],dtype=np.float32)\n",
    "for i in range(num_images):\n",
    "    final_images[i,0] = out_images[i]\n",
    "    final_masks[i,0] = out_nodemasks[i]\n",
    "\n",
    "rand_i = np.random.choice(range(num_images),size=num_images,replace=False)\n",
    "test_i = int(0.2*num_images)\n",
    "np.save(working_path+\"trainImages.npy\",final_images[rand_i[test_i:]])\n",
    "np.save(working_path+\"trainMasks.npy\",final_masks[rand_i[test_i:]])\n",
    "np.save(working_path+\"testImages.npy\",final_images[rand_i[:test_i]])\n",
    "np.save(working_path+\"testMasks.npy\",final_masks[rand_i[:test_i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "working_path = \"/home/jonathan/tutorial/\"\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "\n",
    "img_rows = 512\n",
    "img_cols = 512\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_np(y_true,y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def get_unet():\n",
    "    inputs = Input((1,img_rows, img_cols))\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(pool2)\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(pool3)\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(pool4)\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(conv5)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(up6)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(up7)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(up8)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(up9)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv9)\n",
    "\n",
    "    conv10 = Convolution2D(1, 1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_predict(use_existing):\n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing train data...')\n",
    "    print('-'*30)\n",
    "    imgs_train = np.load(working_path+\"trainImages.npy\").astype(np.float32)\n",
    "    imgs_mask_train = np.load(working_path+\"trainMasks.npy\").astype(np.float32)\n",
    "\n",
    "    imgs_test = np.load(working_path+\"testImages.npy\").astype(np.float32)\n",
    "    imgs_mask_test_true = np.load(working_path+\"testMasks.npy\").astype(np.float32)\n",
    "    \n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "    imgs_train -= mean  # images should already be standardized, but just in case\n",
    "    imgs_train /= std\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-'*30)\n",
    "    model = get_unet()\n",
    "    # Saving weights to unet.hdf5 at checkpoints\n",
    "    model_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)\n",
    "    #\n",
    "    # Should we load existing weights? \n",
    "    # Set argument for call to train_and_predict to true at end of script\n",
    "    if use_existing:\n",
    "        model.load_weights('./unet.hdf5')\n",
    "        \n",
    "    # \n",
    "    # The final results for this tutorial were produced using a multi-GPU\n",
    "    # machine using TitanX's.\n",
    "    # For a home GPU computation benchmark, on my home set up with a GTX970 \n",
    "    # I was able to run 20 epochs with a training set size of 320 and \n",
    "    # batch size of 2 in about an hour. I started getting reseasonable masks \n",
    "    # after about 3 hours of training. \n",
    "    #\n",
    "    print('-'*30)\n",
    "    print('Fitting model...')\n",
    "    print('-'*30)\n",
    "    model.fit(imgs_train, imgs_mask_train, batch_size=2, nb_epoch=20, verbose=1, shuffle=True,\n",
    "              callbacks=[model_checkpoint])\n",
    "\n",
    "    # loading best weights from training session\n",
    "    print('-'*30)\n",
    "    print('Loading saved weights...')\n",
    "    print('-'*30)\n",
    "    model.load_weights('./unet.hdf5')\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Predicting masks on test data...')\n",
    "    print('-'*30)\n",
    "    num_test = len(imgs_test)\n",
    "    imgs_mask_test = np.ndarray([num_test,1,512,512],dtype=np.float32)\n",
    "    for i in range(num_test):\n",
    "        imgs_mask_test[i] = model.predict([imgs_test[i:i+1]], verbose=0)[0]\n",
    "    np.save('masksTestPredicted.npy', imgs_mask_test)\n",
    "    mean = 0.0\n",
    "    for i in range(num_test):\n",
    "        mean+=dice_coef_np(imgs_mask_test_true[i,0], imgs_mask_test[i,0])\n",
    "    mean/=num_test\n",
    "    print(\"Mean Dice Coeff : \",mean)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_and_predict(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# usage: python classify_nodes.py nodes.npy \n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import StratifiedKFold as KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "import xgboost as xgb\n",
    "\n",
    "def getRegionFromMap(slice_npy):\n",
    "    thr = np.where(slice_npy > np.mean(slice_npy),0.,1.0)\n",
    "    label_image = label(thr)\n",
    "    labels = label_image.astype(int)\n",
    "    regions = regionprops(labels)\n",
    "    return regions\n",
    "\n",
    "def getRegionMetricRow(fname = \"nodules.npy\"):\n",
    "    # fname, numpy array of dimension [#slices, 1, 512, 512] containing the images\n",
    "    seg = np.load(fname)\n",
    "    nslices = seg.shape[0]\n",
    "    \n",
    "    #metrics\n",
    "    totalArea = 0.\n",
    "    avgArea = 0.\n",
    "    maxArea = 0.\n",
    "    avgEcc = 0.\n",
    "    avgEquivlentDiameter = 0.\n",
    "    stdEquivlentDiameter = 0.\n",
    "    weightedX = 0.\n",
    "    weightedY = 0.\n",
    "    numNodes = 0.\n",
    "    numNodesperSlice = 0.\n",
    "    # crude hueristic to filter some bad segmentaitons\n",
    "    # do not allow any nodes to be larger than 10% of the pixels to eliminate background regions\n",
    "    maxAllowedArea = 0.10 * 512 * 512 \n",
    "    \n",
    "    areas = []\n",
    "    eqDiameters = []\n",
    "    for slicen in range(nslices):\n",
    "        regions = getRegionFromMap(seg[slicen,0,:,:])\n",
    "        for region in regions:\n",
    "            if region.area > maxAllowedArea:\n",
    "                continue\n",
    "            totalArea += region.area\n",
    "            areas.append(region.area)\n",
    "            avgEcc += region.eccentricity\n",
    "            avgEquivlentDiameter += region.equivalent_diameter\n",
    "            eqDiameters.append(region.equivalent_diameter)\n",
    "            weightedX += region.centroid[0]*region.area\n",
    "            weightedY += region.centroid[1]*region.area\n",
    "            numNodes += 1\n",
    "            \n",
    "    weightedX = weightedX / totalArea \n",
    "    weightedY = weightedY / totalArea\n",
    "    avgArea = totalArea / numNodes\n",
    "    avgEcc = avgEcc / numNodes\n",
    "    avgEquivlentDiameter = avgEquivlentDiameter / numNodes\n",
    "    stdEquivlentDiameter = np.std(eqDiameters)\n",
    "    \n",
    "    maxArea = max(areas)\n",
    "    \n",
    "    \n",
    "    numNodesperSlice = numNodes*1. / nslices\n",
    "    \n",
    "    \n",
    "    return np.array([avgArea,maxArea,avgEcc,avgEquivlentDiameter,\\\n",
    "                     stdEquivlentDiameter, weightedX, weightedY, numNodes, numNodesperSlice])\n",
    "\n",
    "\n",
    "def createFeatureDataset(nodfiles=None):\n",
    "    if nodfiles == None:\n",
    "        # directory of numpy arrays containing masks for nodules\n",
    "        # found via unet segmentation\n",
    "        noddir = \"/training_set/\" \n",
    "        nodfiles = glob(noddir +\"*npy\")\n",
    "    # dict with mapping between training examples and true labels\n",
    "    # the training set is the output masks from the unet segmentation\n",
    "    truthdata = pickle.load(open(\"truthdict.pkl\",'r'))\n",
    "    numfeatures = 9\n",
    "    feature_array = np.zeros((len(nodfiles),numfeatures))\n",
    "    truth_metric = np.zeros((len(nodfiles)))\n",
    "    \n",
    "    for i,nodfile in enumerate(nodfiles):\n",
    "        patID = nodfile.split(\"_\")[2]\n",
    "        truth_metric[i] = truthdata[int(patID)]\n",
    "        feature_array[i] = getRegionMetricRow(nodfile)\n",
    "    \n",
    "    np.save(\"dataY.npy\", truth_metric)\n",
    "    np.save(\"dataX.npy\", feature_array)\n",
    "\n",
    "import scipy as sp\n",
    "def logloss(act, pred):\n",
    "    epsilon = 1e-15\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "    ll = ll * -1.0/len(act)\n",
    "    return ll\n",
    "\n",
    "\n",
    "def classifyData():\n",
    "    X = np.load(\"dataX.npy\")\n",
    "    Y = np.load(\"dataY.npy\")\n",
    "\n",
    "    kf = KFold(Y, n_folds=3)\n",
    "    y_pred = Y * 0\n",
    "    for train, test in kf:\n",
    "        X_train, X_test, y_train, y_test = X[train,:], X[test,:], Y[train], Y[test]\n",
    "        clf = RF(n_estimators=100, n_jobs=3)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred[test] = clf.predict(X_test)\n",
    "    print classification_report(Y, y_pred, target_names=[\"No Cancer\", \"Cancer\"])\n",
    "    print(\"logloss\",logloss(Y, y_pred))\n",
    "\n",
    "    # All Cancer\n",
    "    print \"Predicting all positive\"\n",
    "    y_pred = np.ones(Y.shape)\n",
    "    print classification_report(Y, y_pred, target_names=[\"No Cancer\", \"Cancer\"])\n",
    "    print(\"logloss\",logloss(Y, y_pred))\n",
    "\n",
    "    # No Cancer\n",
    "    print \"Predicting all negative\"\n",
    "    y_pred = Y*0\n",
    "    print classification_report(Y, y_pred, target_names=[\"No Cancer\", \"Cancer\"])\n",
    "    print(\"logloss\",logloss(Y, y_pred))\n",
    "\n",
    "    # try XGBoost\n",
    "    print (\"XGBoost\")\n",
    "    kf = KFold(Y, n_folds=3)\n",
    "    y_pred = Y * 0\n",
    "    for train, test in kf:\n",
    "        X_train, X_test, y_train, y_test = X[train,:], X[test,:], Y[train], Y[test]\n",
    "        clf = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred[test] = clf.predict(X_test)\n",
    "    print classification_report(Y, y_pred, target_names=[\"No Cancer\", \"Cancer\"])\n",
    "    print(\"logloss\",logloss(Y, y_pred))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from sys import argv  \n",
    "    \n",
    "    getRegionMetricRow(argv[1:])\n",
    "    classifyData()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
